---
title: "Model Fitting - Final Project"
author: "John Tuong"
format: html
execute: 
  warning: false
editor: visual
editor_options: 
  chunk_output_type: console
---

## Modeling: How can we optimize predictions?

### Model Fitting and Selection in Diabetes Risk Prediction

#### Introduction

> As we move from our EDA and understanding our variables, we're now in the phase of fitting and selecting a 'best' model to predict diabetes risk. Throughout this, we'll be using tidymodels and working to split our previous explored data, creating a CV fold split making recipes and identify the 'best' model through model fitting, selection, and tuning by training our model on our training/test split using a 70/30 split. This allows us to assess how our model would perform on unseen data. We'll look to ensure we don't underfit or overfit our model, which both lead to unreliable prediction models. Some of the model trees we'll be using are the classification tree and the random forest tree, non-ensemble and ensemble method, respectively. I'll give an in-depth explanation of those two models when we get to that part of the process. We'll obtain the two best models for each model type, compare both models on the test set and then choose an overall winner!
>
> Due to the rise of diabetes, finding a good prediction model is important because it allows researchers to extract meaningful insights from data, make accurate predictions, which leads to better informed decisions based on patterns found in the data to potentially better approach diabetes risk. Hopefully we can continue to make advances in order tackle these real world issues. With that - let's begin!

### Starting off

> As always, we'll need to load in the necessary packages. In this case we'll be using tidyverse and some others.

```{r}
# Load packages
# install.packages("tidymodels")
# install.packages("ranger")
# install.packages("parsnip")
library(tidyverse)
library(tidymodels)
library(ranger)
library(parsnip)
```


### Splitting the Data

> Now we're going to split the data using a 70/30 test/training split. As well as create a 5 fold CV split.

```{r}
# Splitting data and creating a 5 fold CV
set.seed(11)
diabetes_split <- initial_split(diabetes_data, prop = 0.70, strata = Diabetes_binary)
diabetes_train <- training(diabetes_split)
diabetes_test <- testing(diabetes_split)
diabetes_5_fold <- vfold_cv(diabetes_train, 5)
```

### Fitting MLR Models

> Here we'll start by creating our recipe using 5 predictor variables (high cholesterol, smoker status, sex, physical activity, and education). We can use the same recipe for the classification tree and the random tree forest. Additionally, I have created a subsetted data set using 20% of the total data set in order to trim model fitting time (I didn't do this at the beginning and it took a very long time).

```{r}
# Creating the recipe
diabetes_recipe <- recipe(Diabetes_binary ~ chol + smoke + sex + physical + education, data = diabetes_train) %>%
  step_dummy(chol, smoke, sex, physical, education) %>%
  step_normalize(all_numeric())
```

```{r}
# Creating the subset data set
subset_data <- diabetes_data[sample(1:nrow(diabetes_data), 50736, replace = FALSE),]

# Creating the subset data set
subset_recipe <- recipe(Diabetes_binary ~ chol + smoke + sex + physical + education, data = subset_data) %>%
  step_dummy(chol, smoke, sex, physical, education) %>%
  step_normalize(all_numeric())
```

> Now that we have our recipe we're going to fit a classification tree. In short, a classification tree model, also known as a logistic regression model, is a commonly used model with a binary response. It's used to predict binary outcomes, and in this case its goal is to classify an observation into a category: Diabetes_binary being the observation, yes or no being the categories. It's a supervised learning method and a type of non-ensemble tree. It's a supervised learning method meaning that it uses labeled (specified inputs, called features, and outputs, which are labels) data sets to train models to predict outcomes and recognize patterns. Non-ensemble meaning that this model only makes predictions on a single model rather than combining predictions from multiple models, such as random forest tree models. In order words, its standard algorithm for fitting trees are in a sequential (one split at a time) manner. How it works is it attempts to split up predictor space into regions/subsets. On each region, a different prediction is made. For every possible value of each predictor, it finds the squared error loss based on splitting the data around that point. To build and fit the tree, the splitting process uses recursive binary splitting, a greedy algorithm, to grow on the training data, only stopping when each node has fewer than some minimum number (min_n) of observations or maximum depth (tree_depth) of the tree, both of which are tuning parameters for classification trees. The number of nodes/splits are chosen by using the training/test set or cross validation. Tree are then pruned by cost_complexity pruning (cost_complexty = tune()), another tuning parameter, to not over fit the data. The common metrics to quantify prediction quality are accuracy, misclassification rate, and log-loss. Some advantages of classification trees are that they are easy to interpret and predictors don't need to be scaled. However, some disadvantages are that they small changes in data can change the tree drastically, there's no optimal algorithm for choosing splits that exists, and need to either prune or use cross validation in order to determine the model. With that, we'll now start by setting up and fitting the classification with varying values for the complexity parameter and choose the best model.

```{r}
# Setting up our classification tree model fit.
tree_spec <- decision_tree(cost_complexity = tune(),
                           tree_depth = tune(),
                           min_n = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")

# Creating the workflow
tree_wkf <- workflow() %>%
  add_recipe(diabetes_recipe) %>%
  add_model(tree_spec)

# Fitting the model  grid_regular, creating a hyperparameter grid. Here we're setting the levels of cost_complexity and tree_depth to 5 and 5, respectively, resulting in a total of 25 (5 x 5) different combinations of parameters to be evaluated.
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          min_n(),
                          levels = c(5, 5, 10))

# Model tuning with hyperparameter grid
tree_fits <- tree_wkf %>%
  tune_grid(resamples = diabetes_5_fold,
            grid = tree_grid,
            metrics = metric_set(mn_log_loss))

# Collecting metrics to see which tuning parameter is the best
tree_fits %>%
  collect_metrics() %>%
  filter(.metric == "mn_log_loss") %>%
  arrange(mean)

# Selecting best log-loss
lowest_log_tree <- tree_fits %>%
  select_best(metric = "mn_log_loss")
lowest_log_tree
```

-   While I'm not sure if we were supposed to keep all the parameter values that I used, after about 10 or so runs, I started to keep track of what parameter values I had previously used.
    -   log-loss = 0.404; complexity = 10, depth = 5, min_n = 20
    -   log-loss = 0.399; complexity = 5, depth = 5, min_n = 15
    -   log-loss = 0.404; complexity = 5, depth = 5, min_n = 30
    -   log-loss = 0.394; complexity = 5, depth = 5, min_n = 10
    -   log-loss = 0.394; complexity = 5, depth = 5, min_n = 5
    -   log-loss = 0.394; complexity = 5, depth = 5, min_n = 20
    -   log-loss = 0.404; complexity = 20, depth = 5, min_n = 35
    -   log-loss = 0.404; complexity = 35, depth = 10, min_n = 30
-   So after a while of running and constant tuning, my best log-loss value was 0.394 for the classification model. Now we'll move onto fitting the random forest tree model!

> A random forest tree model, also a supervised learning method, uses something called bootstrapping aggregation to take a train multiple models on a bunch of different subsets of the data and uses replacement (can use the same data points in different models when creating more) and then takes all of the metrics from those models and averages them to obtain the error metric. The model chooses to use a different mix of predictor variables for every time a different subset of data is being trained. Due to this reason it creates much more diversity and reduces variance between model predictions,leading to higher accuracy. Random forest tree models are highly accurate predictive models and can be used for both classification and regression tasks. We might want use a random forest tree model when we: want to reduce over fitting with more diversity (good when we have a decision tree that over fits the data since the average results of many trees could result in a more stable model and higher accuracy), if we have high dimensional data/a large number of features and complex feature relationships (known as feature importance, the model can measure importance of features, making it important to understand which ones have the most impact on predictions). Some disadvantages of using a random forest model is that due to less interpretability (because it's an ensemble, it's hard to interpret compared to a single tree), computational complexity/costs (since there are multiple trees being trained, and if it's a large data set, it takes a lot of computing power to aggregate its predictions). With that, we'll now start by setting up and fitting the random forest tree model.

```{r}
# Setting up our random forest tree model
rf_spec <- rand_forest(mtry = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

# Creating the workflow
rf_wkf <- workflow() %>%
  add_recipe(diabetes_recipe) %>%
  add_model(rf_spec)

# Fitting the model with tune_grid()
rf_fit <- rf_wkf %>%
  tune_grid(resamples = diabetes_5_fold,
            grid = 10,
            metrics = metric_set(mn_log_loss))

# Collecting metrics to see which tuning parameter is the best
rf_fit %>%
  collect_metrics() %>%
  filter(.metric == "mn_log_loss") %>%
  arrange(mean)

# Selecting best log-loss
lowest_rf <- rf_fit %>%
  select_best(metric = "mn_log_loss")
lowest_rf
```

-   This is just to keep track of what values I've used to adjust.

    -   log-loss .373; grid = 2
    -   log-loss .373; mtry = 6, grid = 2
    -   log-loss .373; mtry = 5, grid = 10
    -   log-loss .384; mtry = 1, grid = 10
    -   log-loss .373; mtry = 6, grid = 15

-   For the random forest tree model, our best fit had a log-loss of 0.373.

> Now that we've found the best log-loss for both the classification tree and the random forest tree, we can work on our final model selection. We're going to compare both models on the training data and test on the test set (similar to the method on homework 9 key) and declare which one is better. 

```{r}
# Re-obtaining metrics for each to fit into final model

# Final metric for classification tree Model
best_tree_fit <- tree_wkf %>%
  finalize_workflow(lowest_log_tree) %>% 
  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))
best_tree_fit

# Final metric for random forest tree model
best_rf_fit <- rf_wkf %>%
  finalize_workflow(lowest_rf) %>%
  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))
best_rf_fit

# Obtaining metrics for classification tree
best_tree_fit %>%
  collect_metrics()

# Obtaining metrics for random forest tree
best_rf_fit %>%
  collect_metrics()
```

+ From here we can see the best model is the random forest mode due to the lowest log-loss of 0.373 compared to the classification tree model which output a log-loss of 0.404. So now we'll fit random forest as the best model. 

```{r}
# Fitting better over model - rf model
better_overall_model <- rf_wkf %>%
  finalize_workflow(lowest_rf) %>%
  fit(diabetes_data)
better_overall_model
```

> With that, we have a number of trees of 500, a sample size of 253,860, we've obtained 9 indepedent variables (additional ones because education has multiple levels), an mtry of 7, a target node size of 10, variable importance node of impurity, a splitrule of gini. Now onto our API!