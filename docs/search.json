[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Model Fitting - Final Project",
    "section": "",
    "text": "As we move from our EDA and understanding our variables, we’re now in the phase of fitting and selecting a ‘best’ model to predict diabetes risk. Throughout this, we’ll be using tidymodels and working to split our previous explored data, creating a CV fold split making recipes and identify the ‘best’ model through model fitting, selection, and tuning by training our model on our training/test split using a 70/30 split. This allows us to assess how our model would perform on unseen data. We’ll look to ensure we don’t underfit or overfit our model, which both lead to unreliable prediction models. Some of the model trees we’ll be using are the classification tree and the random forest tree, non-ensemble and ensemble method, respectively. I’ll give an in-depth explanation of those two models when we get to that part of the process. We’ll obtain the two best models for each model type, compare both models on the test set and then choose an overall winner!\nDue to the rise of diabetes, finding a good prediction model is important because it allows researchers to extract meaningful insights from data, make accurate predictions, which leads to better informed decisions based on patterns found in the data to potentially better approach diabetes risk. Hopefully we can continue to make advances in order tackle these real world issues. With that - let’s begin!\n\n\n\n\n\n\nAs always, we’ll need to load in the necessary packages. In this case we’ll be using tidyverse and some others.\n\n\n# Load packages\n# install.packages(\"tidymodels\")\n# install.packages(\"ranger\")\n# install.packages(\"parsnip\")\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ranger)\nlibrary(parsnip)\n\n\nRepeating this step quickly from my EDA to ensure that the data set can be read into my Modeling file.\n\n\n# Then we'll read in our data\ndiabetes_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n\nNow we’ll check the data for missingness and column types and values. There are no signs of any NA values, we’re good to go.\n\n\n# Checking for missingness\ndiabetes_data %&gt;%\n  is.na() %&gt;%\n  colSums()\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n# Checking column type and values\nattributes(diabetes_data)$spec\n\ncols(\n  Diabetes_binary = col_double(),\n  HighBP = col_double(),\n  HighChol = col_double(),\n  CholCheck = col_double(),\n  BMI = col_double(),\n  Smoker = col_double(),\n  Stroke = col_double(),\n  HeartDiseaseorAttack = col_double(),\n  PhysActivity = col_double(),\n  Fruits = col_double(),\n  Veggies = col_double(),\n  HvyAlcoholConsump = col_double(),\n  AnyHealthcare = col_double(),\n  NoDocbcCost = col_double(),\n  GenHlth = col_double(),\n  MentHlth = col_double(),\n  PhysHlth = col_double(),\n  DiffWalk = col_double(),\n  Sex = col_double(),\n  Age = col_double(),\n  Education = col_double(),\n  Income = col_double()\n)\n\n\n\nNow we’ll clean the data and correct them to their respective type. Yes/No predictors will be factors. Others with multiple categories, non-continuous ones such as General Health, Education, Income, Age (because it’s in groupings of 4) will be ordinal factors and the others (BMI, Mental Health, Physical Health, Age) will become a numeric. Additionally, we’ll assign the each applicable level their respective label. Then we’ll rename them for ease of reference.\n\n\n# Mutating to the correct column types\ndiabetes_data &lt;- diabetes_data %&gt;%\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    BMI = as.numeric(BMI),\n    Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No (Smoker)\", \"Yes (Smoker)\")),\n    Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Veggies = factor(Veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, levels = c(1,2,3,4,5), labels = c(\"excellent\", \"very good\", \"good\", \"fair\", \"poor\"), ordered = T),\n    MentHlth = as.numeric(MentHlth),\n    PhysHlth = as.numeric(PhysHlth),\n    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Sex = factor(Sex, levels = c(0, 1), labels = c(\"female\", \"male\")),\n    Age = as.numeric(Age),\n    Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"no school or only kindergarten\", \"elementary\", \"some high school\", \"high school or GED\", \"some college or technical school\", \"college grad\")),\n    Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"less than $10K\", \"$10K to $15K\", \"$15K to $20K\", \"$20K to $25K\", \"$25K to $35K\", \"$35K to $50K\", \"$50K to $75K\", \"more than $75K\"), ordered = T)\n  )\n\n# Checking column types\nstr(diabetes_data)\n\ntibble [253,680 × 22] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"No (Smoker)\",..: 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Ord.factor w/ 5 levels \"excellent\"&lt;\"very good\"&lt;..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"female\",\"male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"no school or only kindergarten\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Ord.factor w/ 8 levels \"less than $10K\"&lt;..: 3 1 8 6 4 8 7 4 1 3 ...\n\n\n\n# Renaming for ease of reference\ndiabetes_data &lt;- diabetes_data %&gt;%\n  rename('bp' = 'HighBP',\n         'chol' = 'HighChol',\n         'cholcheck' = 'CholCheck',\n         'bmi' = 'BMI',\n         'smoke' = 'Smoker',\n         'stroke' = 'Stroke',\n         'heartdiseaseorattack' = 'HeartDiseaseorAttack',\n         'physical' = 'PhysActivity',\n         'fruit' = 'Fruits',\n         'veg' = 'Veggies',\n         'alc' = 'HvyAlcoholConsump',\n         'healthcare' = 'AnyHealthcare',\n         'doc' = 'NoDocbcCost',\n         'genhlth' = 'GenHlth',\n         'menthlth' = 'MentHlth',\n         'physhlth' = 'PhysHlth',\n         'diffwalk' = 'DiffWalk',\n         'sex' = 'Sex',\n         'age' = 'Age',\n         'education' = 'Education',\n         'income' = 'Income')\n\n\n\n\n\nNow we’re going to split the data using a 70/30 test/training split. As well as create a 5 fold CV split.\n\n\n# Splitting data and creating a 5 fold CV\nset.seed(11)\ndiabetes_split &lt;- initial_split(diabetes_data, prop = 0.70, strata = Diabetes_binary)\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\ndiabetes_5_fold &lt;- vfold_cv(diabetes_train, 5)\n\n\n\n\n\nHere we’ll start by creating our recipe using 5 predictor variables (high cholesterol, smoker status, sex, physical activity, and education). We can use the same recipe for the classification tree and the random tree forest. Additionally, I have created a subsetted data set using 20% of the total data set in order to trim model fitting time (I didn’t do this at the beginning and it took a very long time).\n\n\n# Creating the recipe\ndiabetes_recipe &lt;- recipe(Diabetes_binary ~ chol + smoke + sex + physical + education, data = diabetes_train) %&gt;%\n  step_dummy(chol, smoke, sex, physical, education) %&gt;%\n  step_normalize(all_numeric())\n\n\n# Creating the subset data set\nsubset_data &lt;- diabetes_data[sample(1:nrow(diabetes_data), 50736, replace = FALSE),]\n\n# Creating the subset data set\nsubset_recipe &lt;- recipe(Diabetes_binary ~ chol + smoke + sex + physical + education, data = subset_data) %&gt;%\n  step_dummy(chol, smoke, sex, physical, education) %&gt;%\n  step_normalize(all_numeric())\n\n\nNow that we have our recipe we’re going to fit a classification tree. In short, a classification tree model, also known as a logistic regression model, is a commonly used model with a binary response. It’s used to predict binary outcomes, and in this case its goal is to classify an observation into a category: Diabetes_binary being the observation, yes or no being the categories. It’s a supervised learning method and a type of non-ensemble tree. It’s a supervised learning method meaning that it uses labeled (specified inputs, called features, and outputs, which are labels) data sets to train models to predict outcomes and recognize patterns. Non-ensemble meaning that this model only makes predictions on a single model rather than combining predictions from multiple models, such as random forest tree models. In order words, its standard algorithm for fitting trees are in a sequential (one split at a time) manner. How it works is it attempts to split up predictor space into regions/subsets. On each region, a different prediction is made. For every possible value of each predictor, it finds the squared error loss based on splitting the data around that point. To build and fit the tree, the splitting process uses recursive binary splitting, a greedy algorithm, to grow on the training data, only stopping when each node has fewer than some minimum number (min_n) of observations or maximum depth (tree_depth) of the tree, both of which are tuning parameters for classification trees. The number of nodes/splits are chosen by using the training/test set or cross validation. Tree are then pruned by cost_complexity pruning (cost_complexty = tune()), another tuning parameter, to not over fit the data. The common metrics to quantify prediction quality are accuracy, misclassification rate, and log-loss. Some advantages of classification trees are that they are easy to interpret and predictors don’t need to be scaled. However, some disadvantages are that they small changes in data can change the tree drastically, there’s no optimal algorithm for choosing splits that exists, and need to either prune or use cross validation in order to determine the model. With that, we’ll now start by setting up and fitting the classification with varying values for the complexity parameter and choose the best model.\n\n\n# Setting up our classification tree model fit.\ntree_spec &lt;- decision_tree(cost_complexity = tune(),\n                           tree_depth = tune(),\n                           min_n = tune()) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n# Creating the workflow\ntree_wkf &lt;- workflow() %&gt;%\n  add_recipe(diabetes_recipe) %&gt;%\n  add_model(tree_spec)\n\n# Fitting the model  grid_regular, creating a hyperparameter grid. Here we're setting the levels of cost_complexity and tree_depth to 5 and 5, respectively, resulting in a total of 25 (5 x 5) different combinations of parameters to be evaluated.\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          min_n(),\n                          levels = c(5, 5, 10))\n\n# Model tuning with hyperparameter grid\ntree_fits &lt;- tree_wkf %&gt;%\n  tune_grid(resamples = diabetes_5_fold,\n            grid = tree_grid,\n            metrics = metric_set(mn_log_loss))\n\n# Collecting metrics to see which tuning parameter is the best\ntree_fits %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"mn_log_loss\") %&gt;%\n  arrange(mean)\n\n# A tibble: 250 × 9\n   cost_complexity tree_depth min_n .metric     .estimator  mean     n std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1    0.0000000001         11     2 mn_log_loss binary     0.394     5 0.00644\n 2    0.0000000178         11     2 mn_log_loss binary     0.394     5 0.00644\n 3    0.00000316           11     2 mn_log_loss binary     0.394     5 0.00644\n 4    0.0000000001         15     2 mn_log_loss binary     0.394     5 0.00644\n 5    0.0000000178         15     2 mn_log_loss binary     0.394     5 0.00644\n 6    0.00000316           15     2 mn_log_loss binary     0.394     5 0.00644\n 7    0.0000000001         11     6 mn_log_loss binary     0.394     5 0.00644\n 8    0.0000000178         11     6 mn_log_loss binary     0.394     5 0.00644\n 9    0.00000316           11     6 mn_log_loss binary     0.394     5 0.00644\n10    0.0000000001         15     6 mn_log_loss binary     0.394     5 0.00644\n# ℹ 240 more rows\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n# Selecting best log-loss\nlowest_log_tree &lt;- tree_fits %&gt;%\n  select_best(metric = \"mn_log_loss\")\nlowest_log_tree\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config               \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;                 \n1    0.0000000001         11     2 Preprocessor1_Model016\n\n\n\nWhile I’m not sure if we were supposed to keep all the parameter values that I used, after about 10 or so runs, I started to keep track of what parameter values I had previously used.\n\nlog-loss = 0.404; complexity = 10, depth = 5, min_n = 20\nlog-loss = 0.399; complexity = 5, depth = 5, min_n = 15\nlog-loss = 0.404; complexity = 5, depth = 5, min_n = 30\nlog-loss = 0.394; complexity = 5, depth = 5, min_n = 10\nlog-loss = 0.394; complexity = 5, depth = 5, min_n = 5\nlog-loss = 0.394; complexity = 5, depth = 5, min_n = 20\nlog-loss = 0.404; complexity = 20, depth = 5, min_n = 35\nlog-loss = 0.404; complexity = 35, depth = 10, min_n = 30\n\nSo after a while of running and constant tuning, my best log-loss value was 0.394 for the classification model. Now we’ll move onto fitting the random forest tree model!\n\n\nA random forest tree model, also a supervised learning method, uses something called bootstrapping aggregation to take a train multiple models on a bunch of different subsets of the data and uses replacement (can use the same data points in different models when creating more) and then takes all of the metrics from those models and averages them to obtain the error metric. The model chooses to use a different mix of predictor variables for every time a different subset of data is being trained. Due to this reason it creates much more diversity and reduces variance between model predictions,leading to higher accuracy. Random forest tree models are highly accurate predictive models and can be used for both classification and regression tasks. We might want use a random forest tree model when we: want to reduce over fitting with more diversity (good when we have a decision tree that over fits the data since the average results of many trees could result in a more stable model and higher accuracy), if we have high dimensional data/a large number of features and complex feature relationships (known as feature importance, the model can measure importance of features, making it important to understand which ones have the most impact on predictions). Some disadvantages of using a random forest model is that due to less interpretability (because it’s an ensemble, it’s hard to interpret compared to a single tree), computational complexity/costs (since there are multiple trees being trained, and if it’s a large data set, it takes a lot of computing power to aggregate its predictions). With that, we’ll now start by setting up and fitting the random forest tree model.\n\n\n# Setting up our random forest tree model\nrf_spec &lt;- rand_forest(mtry = tune()) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n# Creating the workflow\nrf_wkf &lt;- workflow() %&gt;%\n  add_recipe(diabetes_recipe) %&gt;%\n  add_model(rf_spec)\n\n# Fitting the model with tune_grid()\nrf_fit &lt;- rf_wkf %&gt;%\n  tune_grid(resamples = diabetes_5_fold,\n            grid = 10,\n            metrics = metric_set(mn_log_loss))\n\n# Collecting metrics to see which tuning parameter is the best\nrf_fit %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"mn_log_loss\") %&gt;%\n  arrange(mean)\n\n# A tibble: 8 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     6 mn_log_loss binary     0.373     5 0.000639 Preprocessor1_Model8\n2     7 mn_log_loss binary     0.373     5 0.000636 Preprocessor1_Model6\n3     5 mn_log_loss binary     0.373     5 0.000645 Preprocessor1_Model5\n4     8 mn_log_loss binary     0.373     5 0.000629 Preprocessor1_Model4\n5     9 mn_log_loss binary     0.373     5 0.000605 Preprocessor1_Model7\n6     4 mn_log_loss binary     0.373     5 0.000645 Preprocessor1_Model1\n7     3 mn_log_loss binary     0.373     5 0.000624 Preprocessor1_Model2\n8     2 mn_log_loss binary     0.375     5 0.000544 Preprocessor1_Model3\n\n# Selecting best log-loss\nlowest_rf &lt;- rf_fit %&gt;%\n  select_best(metric = \"mn_log_loss\")\nlowest_rf\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     6 Preprocessor1_Model8\n\n\n\nThis is just to keep track of what values I’ve used to adjust.\n\nlog-loss .373; grid = 2\nlog-loss .373; mtry = 6, grid = 2\nlog-loss .373; mtry = 5, grid = 10\nlog-loss .384; mtry = 1, grid = 10\nlog-loss .373; mtry = 6, grid = 15\n\nFor the random forest tree model, our best fit had a log-loss of 0.373.\n\n\nNow that we’ve found the best log-loss for both the classification tree and the random forest tree, we can work on our final model selection. We’re going to compare both models on the training data and test on the test set (similar to the method on homework 9 key) and declare which one is better.\n\n\n# Re-obtaining metrics for each to fit into final model\n\n# Final metric for classification tree Model\nbest_tree_fit &lt;- tree_wkf %&gt;%\n  finalize_workflow(lowest_log_tree) %&gt;% \n  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))\nbest_tree_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177575/76105]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n# Final metric for random forest tree model\nbest_rf_fit &lt;- rf_wkf %&gt;%\n  finalize_workflow(lowest_rf) %&gt;%\n  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))\nbest_rf_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177575/76105]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n# Obtaining metrics for classification tree\nbest_tree_fit %&gt;%\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.404 Preprocessor1_Model1\n\n# Obtaining metrics for random forest tree\nbest_rf_fit %&gt;%\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.373 Preprocessor1_Model1\n\n\n\nFrom here we can see the best model is the random forest mode due to the lowest log-loss of 0.373 compared to the classification tree model which output a log-loss of 0.404. So now we’ll fit random forest as the best model.\n\n\n# Fitting better over model - rf model\nbetter_overall_model &lt;- rf_wkf %&gt;%\n  finalize_workflow(lowest_rf) %&gt;%\n  fit(diabetes_data)\nbetter_overall_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~6L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      253680 \nNumber of independent variables:  9 \nMtry:                             6 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1123034 \n\n\n\nWith that, we have a number of trees of 500, a sample size of 253,860, we’ve obtained 9 indepedent variables (additional ones because education has multiple levels), an mtry of 7, a target node size of 10, variable importance node of impurity, a splitrule of gini. Now onto our API!"
  },
  {
    "objectID": "Modeling.html#modeling-how-can-we-optimize-predictions",
    "href": "Modeling.html#modeling-how-can-we-optimize-predictions",
    "title": "Model Fitting - Final Project",
    "section": "",
    "text": "As we move from our EDA and understanding our variables, we’re now in the phase of fitting and selecting a ‘best’ model to predict diabetes risk. Throughout this, we’ll be using tidymodels and working to split our previous explored data, creating a CV fold split making recipes and identify the ‘best’ model through model fitting, selection, and tuning by training our model on our training/test split using a 70/30 split. This allows us to assess how our model would perform on unseen data. We’ll look to ensure we don’t underfit or overfit our model, which both lead to unreliable prediction models. Some of the model trees we’ll be using are the classification tree and the random forest tree, non-ensemble and ensemble method, respectively. I’ll give an in-depth explanation of those two models when we get to that part of the process. We’ll obtain the two best models for each model type, compare both models on the test set and then choose an overall winner!\nDue to the rise of diabetes, finding a good prediction model is important because it allows researchers to extract meaningful insights from data, make accurate predictions, which leads to better informed decisions based on patterns found in the data to potentially better approach diabetes risk. Hopefully we can continue to make advances in order tackle these real world issues. With that - let’s begin!\n\n\n\n\n\n\nAs always, we’ll need to load in the necessary packages. In this case we’ll be using tidyverse and some others.\n\n\n# Load packages\n# install.packages(\"tidymodels\")\n# install.packages(\"ranger\")\n# install.packages(\"parsnip\")\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ranger)\nlibrary(parsnip)\n\n\nRepeating this step quickly from my EDA to ensure that the data set can be read into my Modeling file.\n\n\n# Then we'll read in our data\ndiabetes_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n\nNow we’ll check the data for missingness and column types and values. There are no signs of any NA values, we’re good to go.\n\n\n# Checking for missingness\ndiabetes_data %&gt;%\n  is.na() %&gt;%\n  colSums()\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n# Checking column type and values\nattributes(diabetes_data)$spec\n\ncols(\n  Diabetes_binary = col_double(),\n  HighBP = col_double(),\n  HighChol = col_double(),\n  CholCheck = col_double(),\n  BMI = col_double(),\n  Smoker = col_double(),\n  Stroke = col_double(),\n  HeartDiseaseorAttack = col_double(),\n  PhysActivity = col_double(),\n  Fruits = col_double(),\n  Veggies = col_double(),\n  HvyAlcoholConsump = col_double(),\n  AnyHealthcare = col_double(),\n  NoDocbcCost = col_double(),\n  GenHlth = col_double(),\n  MentHlth = col_double(),\n  PhysHlth = col_double(),\n  DiffWalk = col_double(),\n  Sex = col_double(),\n  Age = col_double(),\n  Education = col_double(),\n  Income = col_double()\n)\n\n\n\nNow we’ll clean the data and correct them to their respective type. Yes/No predictors will be factors. Others with multiple categories, non-continuous ones such as General Health, Education, Income, Age (because it’s in groupings of 4) will be ordinal factors and the others (BMI, Mental Health, Physical Health, Age) will become a numeric. Additionally, we’ll assign the each applicable level their respective label. Then we’ll rename them for ease of reference.\n\n\n# Mutating to the correct column types\ndiabetes_data &lt;- diabetes_data %&gt;%\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    BMI = as.numeric(BMI),\n    Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No (Smoker)\", \"Yes (Smoker)\")),\n    Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Veggies = factor(Veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, levels = c(1,2,3,4,5), labels = c(\"excellent\", \"very good\", \"good\", \"fair\", \"poor\"), ordered = T),\n    MentHlth = as.numeric(MentHlth),\n    PhysHlth = as.numeric(PhysHlth),\n    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Sex = factor(Sex, levels = c(0, 1), labels = c(\"female\", \"male\")),\n    Age = as.numeric(Age),\n    Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"no school or only kindergarten\", \"elementary\", \"some high school\", \"high school or GED\", \"some college or technical school\", \"college grad\")),\n    Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"less than $10K\", \"$10K to $15K\", \"$15K to $20K\", \"$20K to $25K\", \"$25K to $35K\", \"$35K to $50K\", \"$50K to $75K\", \"more than $75K\"), ordered = T)\n  )\n\n# Checking column types\nstr(diabetes_data)\n\ntibble [253,680 × 22] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"No (Smoker)\",..: 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Ord.factor w/ 5 levels \"excellent\"&lt;\"very good\"&lt;..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"female\",\"male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"no school or only kindergarten\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Ord.factor w/ 8 levels \"less than $10K\"&lt;..: 3 1 8 6 4 8 7 4 1 3 ...\n\n\n\n# Renaming for ease of reference\ndiabetes_data &lt;- diabetes_data %&gt;%\n  rename('bp' = 'HighBP',\n         'chol' = 'HighChol',\n         'cholcheck' = 'CholCheck',\n         'bmi' = 'BMI',\n         'smoke' = 'Smoker',\n         'stroke' = 'Stroke',\n         'heartdiseaseorattack' = 'HeartDiseaseorAttack',\n         'physical' = 'PhysActivity',\n         'fruit' = 'Fruits',\n         'veg' = 'Veggies',\n         'alc' = 'HvyAlcoholConsump',\n         'healthcare' = 'AnyHealthcare',\n         'doc' = 'NoDocbcCost',\n         'genhlth' = 'GenHlth',\n         'menthlth' = 'MentHlth',\n         'physhlth' = 'PhysHlth',\n         'diffwalk' = 'DiffWalk',\n         'sex' = 'Sex',\n         'age' = 'Age',\n         'education' = 'Education',\n         'income' = 'Income')\n\n\n\n\n\nNow we’re going to split the data using a 70/30 test/training split. As well as create a 5 fold CV split.\n\n\n# Splitting data and creating a 5 fold CV\nset.seed(11)\ndiabetes_split &lt;- initial_split(diabetes_data, prop = 0.70, strata = Diabetes_binary)\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\ndiabetes_5_fold &lt;- vfold_cv(diabetes_train, 5)\n\n\n\n\n\nHere we’ll start by creating our recipe using 5 predictor variables (high cholesterol, smoker status, sex, physical activity, and education). We can use the same recipe for the classification tree and the random tree forest. Additionally, I have created a subsetted data set using 20% of the total data set in order to trim model fitting time (I didn’t do this at the beginning and it took a very long time).\n\n\n# Creating the recipe\ndiabetes_recipe &lt;- recipe(Diabetes_binary ~ chol + smoke + sex + physical + education, data = diabetes_train) %&gt;%\n  step_dummy(chol, smoke, sex, physical, education) %&gt;%\n  step_normalize(all_numeric())\n\n\n# Creating the subset data set\nsubset_data &lt;- diabetes_data[sample(1:nrow(diabetes_data), 50736, replace = FALSE),]\n\n# Creating the subset data set\nsubset_recipe &lt;- recipe(Diabetes_binary ~ chol + smoke + sex + physical + education, data = subset_data) %&gt;%\n  step_dummy(chol, smoke, sex, physical, education) %&gt;%\n  step_normalize(all_numeric())\n\n\nNow that we have our recipe we’re going to fit a classification tree. In short, a classification tree model, also known as a logistic regression model, is a commonly used model with a binary response. It’s used to predict binary outcomes, and in this case its goal is to classify an observation into a category: Diabetes_binary being the observation, yes or no being the categories. It’s a supervised learning method and a type of non-ensemble tree. It’s a supervised learning method meaning that it uses labeled (specified inputs, called features, and outputs, which are labels) data sets to train models to predict outcomes and recognize patterns. Non-ensemble meaning that this model only makes predictions on a single model rather than combining predictions from multiple models, such as random forest tree models. In order words, its standard algorithm for fitting trees are in a sequential (one split at a time) manner. How it works is it attempts to split up predictor space into regions/subsets. On each region, a different prediction is made. For every possible value of each predictor, it finds the squared error loss based on splitting the data around that point. To build and fit the tree, the splitting process uses recursive binary splitting, a greedy algorithm, to grow on the training data, only stopping when each node has fewer than some minimum number (min_n) of observations or maximum depth (tree_depth) of the tree, both of which are tuning parameters for classification trees. The number of nodes/splits are chosen by using the training/test set or cross validation. Tree are then pruned by cost_complexity pruning (cost_complexty = tune()), another tuning parameter, to not over fit the data. The common metrics to quantify prediction quality are accuracy, misclassification rate, and log-loss. Some advantages of classification trees are that they are easy to interpret and predictors don’t need to be scaled. However, some disadvantages are that they small changes in data can change the tree drastically, there’s no optimal algorithm for choosing splits that exists, and need to either prune or use cross validation in order to determine the model. With that, we’ll now start by setting up and fitting the classification with varying values for the complexity parameter and choose the best model.\n\n\n# Setting up our classification tree model fit.\ntree_spec &lt;- decision_tree(cost_complexity = tune(),\n                           tree_depth = tune(),\n                           min_n = tune()) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n# Creating the workflow\ntree_wkf &lt;- workflow() %&gt;%\n  add_recipe(diabetes_recipe) %&gt;%\n  add_model(tree_spec)\n\n# Fitting the model  grid_regular, creating a hyperparameter grid. Here we're setting the levels of cost_complexity and tree_depth to 5 and 5, respectively, resulting in a total of 25 (5 x 5) different combinations of parameters to be evaluated.\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          min_n(),\n                          levels = c(5, 5, 10))\n\n# Model tuning with hyperparameter grid\ntree_fits &lt;- tree_wkf %&gt;%\n  tune_grid(resamples = diabetes_5_fold,\n            grid = tree_grid,\n            metrics = metric_set(mn_log_loss))\n\n# Collecting metrics to see which tuning parameter is the best\ntree_fits %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"mn_log_loss\") %&gt;%\n  arrange(mean)\n\n# A tibble: 250 × 9\n   cost_complexity tree_depth min_n .metric     .estimator  mean     n std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1    0.0000000001         11     2 mn_log_loss binary     0.394     5 0.00644\n 2    0.0000000178         11     2 mn_log_loss binary     0.394     5 0.00644\n 3    0.00000316           11     2 mn_log_loss binary     0.394     5 0.00644\n 4    0.0000000001         15     2 mn_log_loss binary     0.394     5 0.00644\n 5    0.0000000178         15     2 mn_log_loss binary     0.394     5 0.00644\n 6    0.00000316           15     2 mn_log_loss binary     0.394     5 0.00644\n 7    0.0000000001         11     6 mn_log_loss binary     0.394     5 0.00644\n 8    0.0000000178         11     6 mn_log_loss binary     0.394     5 0.00644\n 9    0.00000316           11     6 mn_log_loss binary     0.394     5 0.00644\n10    0.0000000001         15     6 mn_log_loss binary     0.394     5 0.00644\n# ℹ 240 more rows\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n# Selecting best log-loss\nlowest_log_tree &lt;- tree_fits %&gt;%\n  select_best(metric = \"mn_log_loss\")\nlowest_log_tree\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config               \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;                 \n1    0.0000000001         11     2 Preprocessor1_Model016\n\n\n\nWhile I’m not sure if we were supposed to keep all the parameter values that I used, after about 10 or so runs, I started to keep track of what parameter values I had previously used.\n\nlog-loss = 0.404; complexity = 10, depth = 5, min_n = 20\nlog-loss = 0.399; complexity = 5, depth = 5, min_n = 15\nlog-loss = 0.404; complexity = 5, depth = 5, min_n = 30\nlog-loss = 0.394; complexity = 5, depth = 5, min_n = 10\nlog-loss = 0.394; complexity = 5, depth = 5, min_n = 5\nlog-loss = 0.394; complexity = 5, depth = 5, min_n = 20\nlog-loss = 0.404; complexity = 20, depth = 5, min_n = 35\nlog-loss = 0.404; complexity = 35, depth = 10, min_n = 30\n\nSo after a while of running and constant tuning, my best log-loss value was 0.394 for the classification model. Now we’ll move onto fitting the random forest tree model!\n\n\nA random forest tree model, also a supervised learning method, uses something called bootstrapping aggregation to take a train multiple models on a bunch of different subsets of the data and uses replacement (can use the same data points in different models when creating more) and then takes all of the metrics from those models and averages them to obtain the error metric. The model chooses to use a different mix of predictor variables for every time a different subset of data is being trained. Due to this reason it creates much more diversity and reduces variance between model predictions,leading to higher accuracy. Random forest tree models are highly accurate predictive models and can be used for both classification and regression tasks. We might want use a random forest tree model when we: want to reduce over fitting with more diversity (good when we have a decision tree that over fits the data since the average results of many trees could result in a more stable model and higher accuracy), if we have high dimensional data/a large number of features and complex feature relationships (known as feature importance, the model can measure importance of features, making it important to understand which ones have the most impact on predictions). Some disadvantages of using a random forest model is that due to less interpretability (because it’s an ensemble, it’s hard to interpret compared to a single tree), computational complexity/costs (since there are multiple trees being trained, and if it’s a large data set, it takes a lot of computing power to aggregate its predictions). With that, we’ll now start by setting up and fitting the random forest tree model.\n\n\n# Setting up our random forest tree model\nrf_spec &lt;- rand_forest(mtry = tune()) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n# Creating the workflow\nrf_wkf &lt;- workflow() %&gt;%\n  add_recipe(diabetes_recipe) %&gt;%\n  add_model(rf_spec)\n\n# Fitting the model with tune_grid()\nrf_fit &lt;- rf_wkf %&gt;%\n  tune_grid(resamples = diabetes_5_fold,\n            grid = 10,\n            metrics = metric_set(mn_log_loss))\n\n# Collecting metrics to see which tuning parameter is the best\nrf_fit %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"mn_log_loss\") %&gt;%\n  arrange(mean)\n\n# A tibble: 8 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     6 mn_log_loss binary     0.373     5 0.000639 Preprocessor1_Model8\n2     7 mn_log_loss binary     0.373     5 0.000636 Preprocessor1_Model6\n3     5 mn_log_loss binary     0.373     5 0.000645 Preprocessor1_Model5\n4     8 mn_log_loss binary     0.373     5 0.000629 Preprocessor1_Model4\n5     9 mn_log_loss binary     0.373     5 0.000605 Preprocessor1_Model7\n6     4 mn_log_loss binary     0.373     5 0.000645 Preprocessor1_Model1\n7     3 mn_log_loss binary     0.373     5 0.000624 Preprocessor1_Model2\n8     2 mn_log_loss binary     0.375     5 0.000544 Preprocessor1_Model3\n\n# Selecting best log-loss\nlowest_rf &lt;- rf_fit %&gt;%\n  select_best(metric = \"mn_log_loss\")\nlowest_rf\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     6 Preprocessor1_Model8\n\n\n\nThis is just to keep track of what values I’ve used to adjust.\n\nlog-loss .373; grid = 2\nlog-loss .373; mtry = 6, grid = 2\nlog-loss .373; mtry = 5, grid = 10\nlog-loss .384; mtry = 1, grid = 10\nlog-loss .373; mtry = 6, grid = 15\n\nFor the random forest tree model, our best fit had a log-loss of 0.373.\n\n\nNow that we’ve found the best log-loss for both the classification tree and the random forest tree, we can work on our final model selection. We’re going to compare both models on the training data and test on the test set (similar to the method on homework 9 key) and declare which one is better.\n\n\n# Re-obtaining metrics for each to fit into final model\n\n# Final metric for classification tree Model\nbest_tree_fit &lt;- tree_wkf %&gt;%\n  finalize_workflow(lowest_log_tree) %&gt;% \n  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))\nbest_tree_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177575/76105]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n# Final metric for random forest tree model\nbest_rf_fit &lt;- rf_wkf %&gt;%\n  finalize_workflow(lowest_rf) %&gt;%\n  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))\nbest_rf_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177575/76105]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n# Obtaining metrics for classification tree\nbest_tree_fit %&gt;%\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.404 Preprocessor1_Model1\n\n# Obtaining metrics for random forest tree\nbest_rf_fit %&gt;%\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.373 Preprocessor1_Model1\n\n\n\nFrom here we can see the best model is the random forest mode due to the lowest log-loss of 0.373 compared to the classification tree model which output a log-loss of 0.404. So now we’ll fit random forest as the best model.\n\n\n# Fitting better over model - rf model\nbetter_overall_model &lt;- rf_wkf %&gt;%\n  finalize_workflow(lowest_rf) %&gt;%\n  fit(diabetes_data)\nbetter_overall_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~6L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      253680 \nNumber of independent variables:  9 \nMtry:                             6 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1123034 \n\n\n\nWith that, we have a number of trees of 500, a sample size of 253,860, we’ve obtained 9 indepedent variables (additional ones because education has multiple levels), an mtry of 7, a target node size of 10, variable importance node of impurity, a splitrule of gini. Now onto our API!"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA - Final Project",
    "section": "",
    "text": "Diabetes is a group of chronic diseases that occurs either when the pancreas does not produce enough insulin or when the body cannot effectively use the insulin it produces; insulin is a hormone that regulates blood glucose. In 2022, 14% of adults +18 were living with diabetes, an increase of 7% in 1990. Unmanaged diabetes can be life threatening as it can affect your blood vessels, damaging and rendering different parts of your body useless.\nUsing a diabetes health indicators data set containing health related information from the Behavioral Risk Factor Surveillance System (BRFSS) of the CDC collected from individuals, we’re going to create to a model to predict how different health indicators can assess the risk for diabetes. This data set contains 22 variables with 253,680 data points related to various health conditions, individual behaviors, and demographic information. Diabetes_binary indicates whether or not an individual has diabetes and will be our response variable. Some questions we’ll be investigating specifically are how high cholestorol, smoking, physical activity, sex, and education affects your risk of diabetes.\nThe purpose of this EDA is explore and investigate the relationships between various health indicators and the risk of diabetes. Creating the model to predict that risk is our ultimate goal. This analysis can help us determine what variables and factors are important in assessing our risk for diabetes and provide important insight on how we can better reduce our risk and can better inform our modeling efforts for predictions.\n\n\nHere we will do an EDA for diabetes health indicators by completing the following steps for an EDA:\n\n\nUnderstanding our data, reading it in\nCompleting basic validation\nDetermine the rate of missing values\nClean up the data as needed\nInvestigate the distributions\nApply transformations and repeating previous steps as needed\n\n\n\n\n\n\nFirst we’ll load in whatever packages we need and read in the data accordingly.\n\n\n# Loading packages\nlibrary(tidyverse)\n\n# Then we'll read in our data\ndiabetes_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n\nNow we’ll check the data for missingness and column types and values. There are no signs of any NA values, we’re good to go.\n\n\n# Checking for missingness\ndiabetes_data %&gt;%\n  is.na() %&gt;%\n  colSums()\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n# Checking column type and values\nattributes(diabetes_data)$spec\n\ncols(\n  Diabetes_binary = col_double(),\n  HighBP = col_double(),\n  HighChol = col_double(),\n  CholCheck = col_double(),\n  BMI = col_double(),\n  Smoker = col_double(),\n  Stroke = col_double(),\n  HeartDiseaseorAttack = col_double(),\n  PhysActivity = col_double(),\n  Fruits = col_double(),\n  Veggies = col_double(),\n  HvyAlcoholConsump = col_double(),\n  AnyHealthcare = col_double(),\n  NoDocbcCost = col_double(),\n  GenHlth = col_double(),\n  MentHlth = col_double(),\n  PhysHlth = col_double(),\n  DiffWalk = col_double(),\n  Sex = col_double(),\n  Age = col_double(),\n  Education = col_double(),\n  Income = col_double()\n)\n\n\n\nNow we’ll clean the data and correct them to their respective type. Yes/No predictors will be factors. Others with multiple categories, non-continuous ones such as General Health, Education, Income, Age (because it’s in groupings of 4) will be ordinal factors and the others (BMI, Mental Health, Physical Health, Age) will become a numeric. Additionally, we’ll assign the each applicable level their respective label. Then we’ll rename them for ease of reference.\n\n\n# Mutating to the correct column types\ndiabetes_data &lt;- diabetes_data %&gt;%\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    BMI = as.numeric(BMI),\n    Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No (Smoker)\", \"Yes (Smoker)\")),\n    Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Veggies = factor(Veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, levels = c(1,2,3,4,5), labels = c(\"excellent\", \"very good\", \"good\", \"fair\", \"poor\"), ordered = T),\n    MentHlth = as.numeric(MentHlth),\n    PhysHlth = as.numeric(PhysHlth),\n    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Sex = factor(Sex, levels = c(0, 1), labels = c(\"female\", \"male\")),\n    Age = as.numeric(Age),\n    Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"no school or only kindergarten\", \"elementary\", \"some high school\", \"high school or GED\", \"some college or technical school\", \"college grad\")),\n    Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"less than $10K\", \"$10K to $15K\", \"$15K to $20K\", \"$20K to $25K\", \"$25K to $35K\", \"$35K to $50K\", \"$50K to $75K\", \"more than $75K\"), ordered = T)\n  )\n\n# Checking column types\nstr(diabetes_data)\n\ntibble [253,680 × 22] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"No (Smoker)\",..: 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Ord.factor w/ 5 levels \"excellent\"&lt;\"very good\"&lt;..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"female\",\"male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"no school or only kindergarten\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Ord.factor w/ 8 levels \"less than $10K\"&lt;..: 3 1 8 6 4 8 7 4 1 3 ...\n\n\n\n# Renaming for ease of reference\ndiabetes_data &lt;- diabetes_data %&gt;%\n  rename('bp' = 'HighBP',\n         'chol' = 'HighChol',\n         'cholcheck' = 'CholCheck',\n         'bmi' = 'BMI',\n         'smoke' = 'Smoker',\n         'stroke' = 'Stroke',\n         'heartdiseaseorattack' = 'HeartDiseaseorAttack',\n         'physical' = 'PhysActivity',\n         'fruit' = 'Fruits',\n         'veg' = 'Veggies',\n         'alc' = 'HvyAlcoholConsump',\n         'healthcare' = 'AnyHealthcare',\n         'doc' = 'NoDocbcCost',\n         'genhlth' = 'GenHlth',\n         'menthlth' = 'MentHlth',\n         'physhlth' = 'PhysHlth',\n         'diffwalk' = 'DiffWalk',\n         'sex' = 'Sex',\n         'age' = 'Age',\n         'education' = 'Education',\n         'income' = 'Income')\n\n\n\n\n\nNow we’ll investigate the distributions with some basic numerical summaries and graphical visualizations such as summaries, contingency tables, correlations, and graphs.\n\n\n# Producing a summary to give us the levels and the respective data points for each level and checking for total number of those with diabetes\nsummary(diabetes_data)\n\n Diabetes_binary   bp          chol        cholcheck         bmi       \n No :218334      No :144851   No :146089   No :  9470   Min.   :12.00  \n Yes: 35346      Yes:108829   Yes:107591   Yes:244210   1st Qu.:24.00  \n                                                        Median :27.00  \n                                                        Mean   :28.38  \n                                                        3rd Qu.:31.00  \n                                                        Max.   :98.00  \n                                                                       \n          smoke        stroke       heartdiseaseorattack physical    \n No (Smoker) :141257   No :243388   No :229787           No : 61760  \n Yes (Smoker):112423   Yes: 10292   Yes: 23893           Yes:191920  \n                                                                     \n                                                                     \n                                                                     \n                                                                     \n                                                                     \n fruit         veg          alc         healthcare    doc        \n No : 92782   No : 47839   No :239424   No : 12417   No :232326  \n Yes:160898   Yes:205841   Yes: 14256   Yes:241263   Yes: 21354  \n                                                                 \n                                                                 \n                                                                 \n                                                                 \n                                                                 \n      genhlth         menthlth         physhlth      diffwalk    \n excellent:45299   Min.   : 0.000   Min.   : 0.000   No :211005  \n very good:89084   1st Qu.: 0.000   1st Qu.: 0.000   Yes: 42675  \n good     :75646   Median : 0.000   Median : 0.000               \n fair     :31570   Mean   : 3.185   Mean   : 4.242               \n poor     :12081   3rd Qu.: 2.000   3rd Qu.: 3.000               \n                   Max.   :30.000   Max.   :30.000               \n                                                                 \n     sex              age                                    education     \n female:141974   Min.   : 1.000   no school or only kindergarten  :   174  \n male  :111706   1st Qu.: 6.000   elementary                      :  4043  \n                 Median : 8.000   some high school                :  9478  \n                 Mean   : 8.032   high school or GED              : 62750  \n                 3rd Qu.:10.000   some college or technical school: 69910  \n                 Max.   :13.000   college grad                    :107325  \n                                                                           \n            income     \n more than $75K:90385  \n $50K to $75K  :43219  \n $35K to $50K  :36470  \n $25K to $35K  :25883  \n $20K to $25K  :20135  \n $15K to $20K  :15994  \n (Other)       :21594  \n\ncount(diabetes_data)\n\n# A tibble: 1 × 1\n       n\n   &lt;int&gt;\n1 253680\n\n\n\nHere we can see that there’s a total of 253,680 observations in this data set. Of those observations, there are a total of 35,346 (~14%) people with diabetes. Considering our sample, the mean observation for BMI is 28.38 with minimum or 12.00 and a max value of 98. The max value seems to be an outlier given the rest of the numbers, which could potentially skew the overall distribution and might mean we have to examine it more closely. Both mental and physical health explores how many days the individual felt “not good” in the past 30 days have means of 3.19 and 4.24, respectively. These two variables also have individuals indicating that didn’t good mentally and/or physically for 30 days, which seem to be outliers… something else we might have to take a closer look at. Below we’re going to create some contingency tables to find the most common values for our variables and look at some additional contingency tables.\n\n\n# Most common values from contingency tables\ntable(\"high cholesterol?\" = diabetes_data$chol) # 0, No high chol = more common\n\nhigh cholesterol?\n    No    Yes \n146089 107591 \n\ntable(\"smoking status?\" = diabetes_data$smoke) # 0, No smoking = more common\n\nsmoking status?\n No (Smoker) Yes (Smoker) \n      141257       112423 \n\ntable(\"physical activity?\" = diabetes_data$physical) # 1, yes = more common\n\nphysical activity?\n    No    Yes \n 61760 191920 \n\ntable(\"sex?\" = diabetes_data$sex) # 0, female = more common\n\nsex?\nfemale   male \n141974 111706 \n\ntable(\"education level?\" = diabetes_data$education) # 6, college grade = common\n\neducation level?\n  no school or only kindergarten                       elementary \n                             174                             4043 \n                some high school               high school or GED \n                            9478                            62750 \nsome college or technical school                     college grad \n                           69910                           107325 \n\n\n\n# Contingency tables\ntable(\"diabetes?\" = diabetes_data$Diabetes_binary, \"cholesterol?\" = diabetes_data$chol,\"smoker?\" = diabetes_data$smoke)\n\n, , smoker? = No (Smoker)\n\n         cholesterol?\ndiabetes?    No   Yes\n      No  80908 43320\n      Yes  6125 10904\n\n, , smoker? = Yes (Smoker)\n\n         cholesterol?\ndiabetes?    No   Yes\n      No  53521 40585\n      Yes  5535 12782\n\ntable(\"diabetes?\" = diabetes_data$Diabetes_binary,\"physical activity in 30 days?\" = diabetes_data$physical)\n\n         physical activity in 30 days?\ndiabetes?     No    Yes\n      No   48701 169633\n      Yes  13059  22287\n\ntable(\"diabetes?\" = diabetes_data$Diabetes_binary,\"sex?\" = diabetes_data$sex)\n\n         sex?\ndiabetes? female   male\n      No  123563  94771\n      Yes  18411  16935\n\ntable(\"diabetes?\" = diabetes_data$Diabetes_binary,\"education level?\" = diabetes_data$education)\n\n         education level?\ndiabetes? no school or only kindergarten elementary some high school\n      No                             127       2860             7182\n      Yes                             47       1183             2296\n         education level?\ndiabetes? high school or GED some college or technical school college grad\n      No               51684                            59556        96925\n      Yes              11066                            10354        10400\n\n\n\nLooking at the contingency tables there’s a lot we can take a look at. There’s a total of 12,782 individuals that have high cholesterol and and smoke showcasing a potential relationship between these variables with diabetes prevalence. The results show that those who’ve participate in some form of physical activity that do not have no diabetes are 169,633 individuals. Physical activity is associated with a lower diabetes prevalence, potentially reinforcing how important it is to be active. As for sex, when looking at the distribution for females and males, they’re pretty consistent in terms of total proportion in the data set for those who do or do not have diabetes. For education, it does seem that with more schooling, there’s a decrease of those, proportionally, who have diabetes compared to those who have less schooling. This could reinforce that level of education could be important in diabetes prevention.\n\n\n# Correlations\ndiabetes_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor()\n\n                 bmi    menthlth   physhlth         age\nbmi       1.00000000  0.08531016 0.12114111 -0.03661764\nmenthlth  0.08531016  1.00000000 0.35361887 -0.09206802\nphyshlth  0.12114111  0.35361887 1.00000000  0.09912993\nage      -0.03661764 -0.09206802 0.09912993  1.00000000\n\n\n\nWe can see that BMI, body mass index, have weak positive correlations with both mental (0.0853) and physical health (0.121), meaning that there is only a slight tendency for those with a higher bmi to report “not good” days of mental and physical health. This could potentially mean that BMI alone isn’t a good predictor of both mental and physical health days and we may have to look at other variables. The largest association we see is between mental and physical health with a correlation of 0.354. This is a moderate positive correlation, meaning that they’re somewhat related, indicating that those who reported more subpar mental health days, also reported not great physical health days. This suggests that those who typically experience poor mental health days tend to experience poor physical health days, possibly due to stress, depression, problems with emotions, etc. Next we’ll start creating plots and graphs to observe our data and relationships visually, looking at various distributions and relationship.\n\n\n# Side by Side bar plot of physical activity by diabetes status\nggplot(diabetes_data, aes(x = physical, fill = Diabetes_binary)) +\n  geom_bar(position = \"dodge\") +\n  ggtitle(\"Physical Activity by Diabetes Status\",\n          subtitle = \"Side by Side Bar Plot\") +\n  labs(x = \"Physical Activity\", y = \"Count\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nAs seen above, this represents those who have participated in some physical activity within the last 30 days and diabetes status. Overall, there are less people who do not complete physical activity compared to those who do; however, there proportion of those that don’t do physical activity and have diabetes is higher than those who do complete physical activity and have diabetes. Additionally, there’s a large amount of individuals that do physical activity and don’t have diabetes.\n\n\n# Stacked bar plot of high cholesterol by diabetes with a smokers status facet\nggplot(diabetes_data, aes(x = chol, fill = Diabetes_binary)) +\n  geom_bar(position = \"stack\") +\n  facet_wrap(~ smoke) +\n  ggtitle(\"High Cholesterol by Diabetes Status \\n with Smokers Status Facet\",\n          subtitle = \"Stacked Bar Plot\") +\n  labs(x = \"High Cholesterol\", y = \"Count (Individuals and their Diabetes Status)\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nHere stacked bar plots do not represent proportion but rather total count. Here can see that the total amount of people who don’t smoke and have high cholesterol are higher than those who do smoke, whereas, the other 3 categories have a similar total count. Of the total counts, we can see that those with those high cholesterol, both smokers and non-smokers, have the higher counts of those with diabetes.\n\n\n# Boxplot for mental health and high cholesterol\nggplot(diabetes_data, aes(x = chol, y = physhlth, fill = Diabetes_binary)) +\n  geom_boxplot() +\n  ggtitle(\"High Cholesterol by Subar Physical Health (of Past 30 Days) \\n by Diabetes Status\",\n          subtitle = \"Box Plot\") +\n  labs(x = \"High Cholesterol\", y = \"Subpar Physical Health (by Days)\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nThis shows that those with high cholesterol for both with and without diabetes report more days where their physical health is “not good.” When looking at all categories, we can clearly see the horizontal mean line for those with high cholesterol and diabetes indicating that these individuals tend to report a slightly higher number of “not great” physical days compared to the other categories, which have their mean line at 0. The dots indicate data points that are considered to be outliers.\n\n\n# Filled bar plot of education level by diabetes status\nggplot(diabetes_data, aes(x = education, fill = Diabetes_binary)) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Education Level by Diabetes Status\",\n          subtitle = \"Filled Bar Plot\") +\n  labs(x = \"Education Levels\", y = \"Proportion\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nThis filled bar plot above shows the proportion of education by diabetes status. Red represents the individual has no diabetes and blue represents yes. As seen above, those with no schooling or only kindergarten and elementary schooling have the highest proportions of those with diabetes. As previously mentioned, this could potentially indicate that more schooling or education could help individuals to be better informed regarding diabetes risks.\n\n\n# Filled bar plot sex with diabetes status and general health facet\nggplot(diabetes_data, aes(x = sex, fill = Diabetes_binary)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~ genhlth) +\n  ggtitle(\"Sex by Diabetes Status \\n with General Health Status Facet\",\n          subtitle = \"Filled Bar Plot\") +\n  labs(x = \"Sex\", y = \"Count (Individuals and their Diabetes Status)\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nThe above filled bar plot looks at the proportion of sex and general health status based on diabetes status. For the most part, for each general health status there are roughly a similar amount of those who have diabetes. As seen, those who reported fair and poor health status have the highest proportion of individuals who have diabetes compared to those whom reported excellent or very good health.\n\n\n# Histogram of physical health across diabetes status\nggplot(diabetes_data, aes(x = physhlth, fill = Diabetes_binary)) +\n  geom_histogram(alpha = 0.2, binwidth = 1, position = 'identity') +\n  geom_vline(aes(xintercept = mean(physhlth)),\n            color=\"blue\", linetype = \"dashed\", linewidth = 1) +\n  ggtitle(\"Subpar Physical Health (of Past 30 Days) by \\n across Diabetes Status\",\n          subtitle = \"Histogram Plot\") +\n  labs(x = \"Subpar Physical Health (by Days)\", y = \"Count (Individuals with Diabetes Status)\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nHere the histogram shows that majority, a little less than 150,000, of individuals report 0 “not good” physical health days irregardless of diabetes status, meaning that most people report good physical health days. However it’s interesting to note that the next highest peak is of individuals reporting having up to 30 days of “not good” physical health days. From the dashed blue line, we can see that the mean reported subpar days for the sample is around 4 days of the month.\n\n\n# Hexbin plot of mental health and physical health across diabetes status\nggplot(diabetes_data, aes(x = menthlth, y = physhlth, fill = Diabetes_binary)) +\n  geom_hex() +\n  ggtitle(\"Subpar Mental Health (of Past 30 Days) by \\n Subpar Physical Health (of Past 30 Days) \\n across Diabetes Status\",\n          subtitle = \"Hexbin Plot\") +\n  labs(x = \"Mental Health (by Days)\", y = \"Physical Health (by Days)\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nHere we have a hexbin plot of “not good” mental health days by “not good” physical health days across diabetes status. Each bin is a hexagon and the color represents the number of data points within each bin. For the lower reported mental and physical days (under 10), there is a concentration of those people that have diabetes. There’s a significant spread across both axes, showing variability across the diabetes status. Additionally, there are gaps in certain areas of the plot indicating there’s none or not many data points in the respective bin.\n\n\n# Density plot of BMI by diabetes status\nggplot(diabetes_data, aes(x = bmi, fill = Diabetes_binary)) +\n  geom_density(alpha = 0.6) +\n  geom_vline(aes(xintercept = mean(bmi)),\n            color=\"blue\", linetype = \"dashed\", linewidth = 1) +\n  ggtitle(\"BMI by Diabetes Status\",\n          subtitle = \"Density Plot\") +\n  labs(x = \"BMI\", y = \"Density\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nLast, we have density plot! This density plot visualizes the distribution of BMI values for those with and without diabetes. Here we can see that the peak for both diabetes status are around a BMI of 28, indicating that both groups have most individuals at a BMI of 28. The dashed blue line indicates the mean of around 28 for BMI. When looking at the density for those without diabetes (red), the curve is more tall and narrow compared to the yes diabetes status, indicating that people without diabetes tend to have lower BMIs. Those with diabetes (blue) density curve shifted more to the right indicated that individuals with diabetes tend to have a higher BMI.\n\n\nThat’s a lot of interesting variables and distributions that we’ve looked at! I never really thought about all of the potential variables that could perhaps contribute to or reduce the risk of obtaining diabetes. Now that we’ve completed our EDA, we’re going to create our modeling file that’ll help us predict our diabetes risk.\n\nClick here for the Modeling Page"
  },
  {
    "objectID": "EDA.html#a-deep-dive-what-variables-can-predict-our-risk-of-diabetes",
    "href": "EDA.html#a-deep-dive-what-variables-can-predict-our-risk-of-diabetes",
    "title": "EDA - Final Project",
    "section": "",
    "text": "Diabetes is a group of chronic diseases that occurs either when the pancreas does not produce enough insulin or when the body cannot effectively use the insulin it produces; insulin is a hormone that regulates blood glucose. In 2022, 14% of adults +18 were living with diabetes, an increase of 7% in 1990. Unmanaged diabetes can be life threatening as it can affect your blood vessels, damaging and rendering different parts of your body useless.\nUsing a diabetes health indicators data set containing health related information from the Behavioral Risk Factor Surveillance System (BRFSS) of the CDC collected from individuals, we’re going to create to a model to predict how different health indicators can assess the risk for diabetes. This data set contains 22 variables with 253,680 data points related to various health conditions, individual behaviors, and demographic information. Diabetes_binary indicates whether or not an individual has diabetes and will be our response variable. Some questions we’ll be investigating specifically are how high cholestorol, smoking, physical activity, sex, and education affects your risk of diabetes.\nThe purpose of this EDA is explore and investigate the relationships between various health indicators and the risk of diabetes. Creating the model to predict that risk is our ultimate goal. This analysis can help us determine what variables and factors are important in assessing our risk for diabetes and provide important insight on how we can better reduce our risk and can better inform our modeling efforts for predictions.\n\n\nHere we will do an EDA for diabetes health indicators by completing the following steps for an EDA:\n\n\nUnderstanding our data, reading it in\nCompleting basic validation\nDetermine the rate of missing values\nClean up the data as needed\nInvestigate the distributions\nApply transformations and repeating previous steps as needed\n\n\n\n\n\n\nFirst we’ll load in whatever packages we need and read in the data accordingly.\n\n\n# Loading packages\nlibrary(tidyverse)\n\n# Then we'll read in our data\ndiabetes_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n\nNow we’ll check the data for missingness and column types and values. There are no signs of any NA values, we’re good to go.\n\n\n# Checking for missingness\ndiabetes_data %&gt;%\n  is.na() %&gt;%\n  colSums()\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n# Checking column type and values\nattributes(diabetes_data)$spec\n\ncols(\n  Diabetes_binary = col_double(),\n  HighBP = col_double(),\n  HighChol = col_double(),\n  CholCheck = col_double(),\n  BMI = col_double(),\n  Smoker = col_double(),\n  Stroke = col_double(),\n  HeartDiseaseorAttack = col_double(),\n  PhysActivity = col_double(),\n  Fruits = col_double(),\n  Veggies = col_double(),\n  HvyAlcoholConsump = col_double(),\n  AnyHealthcare = col_double(),\n  NoDocbcCost = col_double(),\n  GenHlth = col_double(),\n  MentHlth = col_double(),\n  PhysHlth = col_double(),\n  DiffWalk = col_double(),\n  Sex = col_double(),\n  Age = col_double(),\n  Education = col_double(),\n  Income = col_double()\n)\n\n\n\nNow we’ll clean the data and correct them to their respective type. Yes/No predictors will be factors. Others with multiple categories, non-continuous ones such as General Health, Education, Income, Age (because it’s in groupings of 4) will be ordinal factors and the others (BMI, Mental Health, Physical Health, Age) will become a numeric. Additionally, we’ll assign the each applicable level their respective label. Then we’ll rename them for ease of reference.\n\n\n# Mutating to the correct column types\ndiabetes_data &lt;- diabetes_data %&gt;%\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    BMI = as.numeric(BMI),\n    Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No (Smoker)\", \"Yes (Smoker)\")),\n    Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Veggies = factor(Veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, levels = c(1,2,3,4,5), labels = c(\"excellent\", \"very good\", \"good\", \"fair\", \"poor\"), ordered = T),\n    MentHlth = as.numeric(MentHlth),\n    PhysHlth = as.numeric(PhysHlth),\n    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Sex = factor(Sex, levels = c(0, 1), labels = c(\"female\", \"male\")),\n    Age = as.numeric(Age),\n    Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"no school or only kindergarten\", \"elementary\", \"some high school\", \"high school or GED\", \"some college or technical school\", \"college grad\")),\n    Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"less than $10K\", \"$10K to $15K\", \"$15K to $20K\", \"$20K to $25K\", \"$25K to $35K\", \"$35K to $50K\", \"$50K to $75K\", \"more than $75K\"), ordered = T)\n  )\n\n# Checking column types\nstr(diabetes_data)\n\ntibble [253,680 × 22] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"No (Smoker)\",..: 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Ord.factor w/ 5 levels \"excellent\"&lt;\"very good\"&lt;..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"female\",\"male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"no school or only kindergarten\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Ord.factor w/ 8 levels \"less than $10K\"&lt;..: 3 1 8 6 4 8 7 4 1 3 ...\n\n\n\n# Renaming for ease of reference\ndiabetes_data &lt;- diabetes_data %&gt;%\n  rename('bp' = 'HighBP',\n         'chol' = 'HighChol',\n         'cholcheck' = 'CholCheck',\n         'bmi' = 'BMI',\n         'smoke' = 'Smoker',\n         'stroke' = 'Stroke',\n         'heartdiseaseorattack' = 'HeartDiseaseorAttack',\n         'physical' = 'PhysActivity',\n         'fruit' = 'Fruits',\n         'veg' = 'Veggies',\n         'alc' = 'HvyAlcoholConsump',\n         'healthcare' = 'AnyHealthcare',\n         'doc' = 'NoDocbcCost',\n         'genhlth' = 'GenHlth',\n         'menthlth' = 'MentHlth',\n         'physhlth' = 'PhysHlth',\n         'diffwalk' = 'DiffWalk',\n         'sex' = 'Sex',\n         'age' = 'Age',\n         'education' = 'Education',\n         'income' = 'Income')\n\n\n\n\n\nNow we’ll investigate the distributions with some basic numerical summaries and graphical visualizations such as summaries, contingency tables, correlations, and graphs.\n\n\n# Producing a summary to give us the levels and the respective data points for each level and checking for total number of those with diabetes\nsummary(diabetes_data)\n\n Diabetes_binary   bp          chol        cholcheck         bmi       \n No :218334      No :144851   No :146089   No :  9470   Min.   :12.00  \n Yes: 35346      Yes:108829   Yes:107591   Yes:244210   1st Qu.:24.00  \n                                                        Median :27.00  \n                                                        Mean   :28.38  \n                                                        3rd Qu.:31.00  \n                                                        Max.   :98.00  \n                                                                       \n          smoke        stroke       heartdiseaseorattack physical    \n No (Smoker) :141257   No :243388   No :229787           No : 61760  \n Yes (Smoker):112423   Yes: 10292   Yes: 23893           Yes:191920  \n                                                                     \n                                                                     \n                                                                     \n                                                                     \n                                                                     \n fruit         veg          alc         healthcare    doc        \n No : 92782   No : 47839   No :239424   No : 12417   No :232326  \n Yes:160898   Yes:205841   Yes: 14256   Yes:241263   Yes: 21354  \n                                                                 \n                                                                 \n                                                                 \n                                                                 \n                                                                 \n      genhlth         menthlth         physhlth      diffwalk    \n excellent:45299   Min.   : 0.000   Min.   : 0.000   No :211005  \n very good:89084   1st Qu.: 0.000   1st Qu.: 0.000   Yes: 42675  \n good     :75646   Median : 0.000   Median : 0.000               \n fair     :31570   Mean   : 3.185   Mean   : 4.242               \n poor     :12081   3rd Qu.: 2.000   3rd Qu.: 3.000               \n                   Max.   :30.000   Max.   :30.000               \n                                                                 \n     sex              age                                    education     \n female:141974   Min.   : 1.000   no school or only kindergarten  :   174  \n male  :111706   1st Qu.: 6.000   elementary                      :  4043  \n                 Median : 8.000   some high school                :  9478  \n                 Mean   : 8.032   high school or GED              : 62750  \n                 3rd Qu.:10.000   some college or technical school: 69910  \n                 Max.   :13.000   college grad                    :107325  \n                                                                           \n            income     \n more than $75K:90385  \n $50K to $75K  :43219  \n $35K to $50K  :36470  \n $25K to $35K  :25883  \n $20K to $25K  :20135  \n $15K to $20K  :15994  \n (Other)       :21594  \n\ncount(diabetes_data)\n\n# A tibble: 1 × 1\n       n\n   &lt;int&gt;\n1 253680\n\n\n\nHere we can see that there’s a total of 253,680 observations in this data set. Of those observations, there are a total of 35,346 (~14%) people with diabetes. Considering our sample, the mean observation for BMI is 28.38 with minimum or 12.00 and a max value of 98. The max value seems to be an outlier given the rest of the numbers, which could potentially skew the overall distribution and might mean we have to examine it more closely. Both mental and physical health explores how many days the individual felt “not good” in the past 30 days have means of 3.19 and 4.24, respectively. These two variables also have individuals indicating that didn’t good mentally and/or physically for 30 days, which seem to be outliers… something else we might have to take a closer look at. Below we’re going to create some contingency tables to find the most common values for our variables and look at some additional contingency tables.\n\n\n# Most common values from contingency tables\ntable(\"high cholesterol?\" = diabetes_data$chol) # 0, No high chol = more common\n\nhigh cholesterol?\n    No    Yes \n146089 107591 \n\ntable(\"smoking status?\" = diabetes_data$smoke) # 0, No smoking = more common\n\nsmoking status?\n No (Smoker) Yes (Smoker) \n      141257       112423 \n\ntable(\"physical activity?\" = diabetes_data$physical) # 1, yes = more common\n\nphysical activity?\n    No    Yes \n 61760 191920 \n\ntable(\"sex?\" = diabetes_data$sex) # 0, female = more common\n\nsex?\nfemale   male \n141974 111706 \n\ntable(\"education level?\" = diabetes_data$education) # 6, college grade = common\n\neducation level?\n  no school or only kindergarten                       elementary \n                             174                             4043 \n                some high school               high school or GED \n                            9478                            62750 \nsome college or technical school                     college grad \n                           69910                           107325 \n\n\n\n# Contingency tables\ntable(\"diabetes?\" = diabetes_data$Diabetes_binary, \"cholesterol?\" = diabetes_data$chol,\"smoker?\" = diabetes_data$smoke)\n\n, , smoker? = No (Smoker)\n\n         cholesterol?\ndiabetes?    No   Yes\n      No  80908 43320\n      Yes  6125 10904\n\n, , smoker? = Yes (Smoker)\n\n         cholesterol?\ndiabetes?    No   Yes\n      No  53521 40585\n      Yes  5535 12782\n\ntable(\"diabetes?\" = diabetes_data$Diabetes_binary,\"physical activity in 30 days?\" = diabetes_data$physical)\n\n         physical activity in 30 days?\ndiabetes?     No    Yes\n      No   48701 169633\n      Yes  13059  22287\n\ntable(\"diabetes?\" = diabetes_data$Diabetes_binary,\"sex?\" = diabetes_data$sex)\n\n         sex?\ndiabetes? female   male\n      No  123563  94771\n      Yes  18411  16935\n\ntable(\"diabetes?\" = diabetes_data$Diabetes_binary,\"education level?\" = diabetes_data$education)\n\n         education level?\ndiabetes? no school or only kindergarten elementary some high school\n      No                             127       2860             7182\n      Yes                             47       1183             2296\n         education level?\ndiabetes? high school or GED some college or technical school college grad\n      No               51684                            59556        96925\n      Yes              11066                            10354        10400\n\n\n\nLooking at the contingency tables there’s a lot we can take a look at. There’s a total of 12,782 individuals that have high cholesterol and and smoke showcasing a potential relationship between these variables with diabetes prevalence. The results show that those who’ve participate in some form of physical activity that do not have no diabetes are 169,633 individuals. Physical activity is associated with a lower diabetes prevalence, potentially reinforcing how important it is to be active. As for sex, when looking at the distribution for females and males, they’re pretty consistent in terms of total proportion in the data set for those who do or do not have diabetes. For education, it does seem that with more schooling, there’s a decrease of those, proportionally, who have diabetes compared to those who have less schooling. This could reinforce that level of education could be important in diabetes prevention.\n\n\n# Correlations\ndiabetes_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor()\n\n                 bmi    menthlth   physhlth         age\nbmi       1.00000000  0.08531016 0.12114111 -0.03661764\nmenthlth  0.08531016  1.00000000 0.35361887 -0.09206802\nphyshlth  0.12114111  0.35361887 1.00000000  0.09912993\nage      -0.03661764 -0.09206802 0.09912993  1.00000000\n\n\n\nWe can see that BMI, body mass index, have weak positive correlations with both mental (0.0853) and physical health (0.121), meaning that there is only a slight tendency for those with a higher bmi to report “not good” days of mental and physical health. This could potentially mean that BMI alone isn’t a good predictor of both mental and physical health days and we may have to look at other variables. The largest association we see is between mental and physical health with a correlation of 0.354. This is a moderate positive correlation, meaning that they’re somewhat related, indicating that those who reported more subpar mental health days, also reported not great physical health days. This suggests that those who typically experience poor mental health days tend to experience poor physical health days, possibly due to stress, depression, problems with emotions, etc. Next we’ll start creating plots and graphs to observe our data and relationships visually, looking at various distributions and relationship.\n\n\n# Side by Side bar plot of physical activity by diabetes status\nggplot(diabetes_data, aes(x = physical, fill = Diabetes_binary)) +\n  geom_bar(position = \"dodge\") +\n  ggtitle(\"Physical Activity by Diabetes Status\",\n          subtitle = \"Side by Side Bar Plot\") +\n  labs(x = \"Physical Activity\", y = \"Count\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nAs seen above, this represents those who have participated in some physical activity within the last 30 days and diabetes status. Overall, there are less people who do not complete physical activity compared to those who do; however, there proportion of those that don’t do physical activity and have diabetes is higher than those who do complete physical activity and have diabetes. Additionally, there’s a large amount of individuals that do physical activity and don’t have diabetes.\n\n\n# Stacked bar plot of high cholesterol by diabetes with a smokers status facet\nggplot(diabetes_data, aes(x = chol, fill = Diabetes_binary)) +\n  geom_bar(position = \"stack\") +\n  facet_wrap(~ smoke) +\n  ggtitle(\"High Cholesterol by Diabetes Status \\n with Smokers Status Facet\",\n          subtitle = \"Stacked Bar Plot\") +\n  labs(x = \"High Cholesterol\", y = \"Count (Individuals and their Diabetes Status)\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nHere stacked bar plots do not represent proportion but rather total count. Here can see that the total amount of people who don’t smoke and have high cholesterol are higher than those who do smoke, whereas, the other 3 categories have a similar total count. Of the total counts, we can see that those with those high cholesterol, both smokers and non-smokers, have the higher counts of those with diabetes.\n\n\n# Boxplot for mental health and high cholesterol\nggplot(diabetes_data, aes(x = chol, y = physhlth, fill = Diabetes_binary)) +\n  geom_boxplot() +\n  ggtitle(\"High Cholesterol by Subar Physical Health (of Past 30 Days) \\n by Diabetes Status\",\n          subtitle = \"Box Plot\") +\n  labs(x = \"High Cholesterol\", y = \"Subpar Physical Health (by Days)\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nThis shows that those with high cholesterol for both with and without diabetes report more days where their physical health is “not good.” When looking at all categories, we can clearly see the horizontal mean line for those with high cholesterol and diabetes indicating that these individuals tend to report a slightly higher number of “not great” physical days compared to the other categories, which have their mean line at 0. The dots indicate data points that are considered to be outliers.\n\n\n# Filled bar plot of education level by diabetes status\nggplot(diabetes_data, aes(x = education, fill = Diabetes_binary)) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Education Level by Diabetes Status\",\n          subtitle = \"Filled Bar Plot\") +\n  labs(x = \"Education Levels\", y = \"Proportion\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nThis filled bar plot above shows the proportion of education by diabetes status. Red represents the individual has no diabetes and blue represents yes. As seen above, those with no schooling or only kindergarten and elementary schooling have the highest proportions of those with diabetes. As previously mentioned, this could potentially indicate that more schooling or education could help individuals to be better informed regarding diabetes risks.\n\n\n# Filled bar plot sex with diabetes status and general health facet\nggplot(diabetes_data, aes(x = sex, fill = Diabetes_binary)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~ genhlth) +\n  ggtitle(\"Sex by Diabetes Status \\n with General Health Status Facet\",\n          subtitle = \"Filled Bar Plot\") +\n  labs(x = \"Sex\", y = \"Count (Individuals and their Diabetes Status)\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nThe above filled bar plot looks at the proportion of sex and general health status based on diabetes status. For the most part, for each general health status there are roughly a similar amount of those who have diabetes. As seen, those who reported fair and poor health status have the highest proportion of individuals who have diabetes compared to those whom reported excellent or very good health.\n\n\n# Histogram of physical health across diabetes status\nggplot(diabetes_data, aes(x = physhlth, fill = Diabetes_binary)) +\n  geom_histogram(alpha = 0.2, binwidth = 1, position = 'identity') +\n  geom_vline(aes(xintercept = mean(physhlth)),\n            color=\"blue\", linetype = \"dashed\", linewidth = 1) +\n  ggtitle(\"Subpar Physical Health (of Past 30 Days) by \\n across Diabetes Status\",\n          subtitle = \"Histogram Plot\") +\n  labs(x = \"Subpar Physical Health (by Days)\", y = \"Count (Individuals with Diabetes Status)\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nHere the histogram shows that majority, a little less than 150,000, of individuals report 0 “not good” physical health days irregardless of diabetes status, meaning that most people report good physical health days. However it’s interesting to note that the next highest peak is of individuals reporting having up to 30 days of “not good” physical health days. From the dashed blue line, we can see that the mean reported subpar days for the sample is around 4 days of the month.\n\n\n# Hexbin plot of mental health and physical health across diabetes status\nggplot(diabetes_data, aes(x = menthlth, y = physhlth, fill = Diabetes_binary)) +\n  geom_hex() +\n  ggtitle(\"Subpar Mental Health (of Past 30 Days) by \\n Subpar Physical Health (of Past 30 Days) \\n across Diabetes Status\",\n          subtitle = \"Hexbin Plot\") +\n  labs(x = \"Mental Health (by Days)\", y = \"Physical Health (by Days)\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nHere we have a hexbin plot of “not good” mental health days by “not good” physical health days across diabetes status. Each bin is a hexagon and the color represents the number of data points within each bin. For the lower reported mental and physical days (under 10), there is a concentration of those people that have diabetes. There’s a significant spread across both axes, showing variability across the diabetes status. Additionally, there are gaps in certain areas of the plot indicating there’s none or not many data points in the respective bin.\n\n\n# Density plot of BMI by diabetes status\nggplot(diabetes_data, aes(x = bmi, fill = Diabetes_binary)) +\n  geom_density(alpha = 0.6) +\n  geom_vline(aes(xintercept = mean(bmi)),\n            color=\"blue\", linetype = \"dashed\", linewidth = 1) +\n  ggtitle(\"BMI by Diabetes Status\",\n          subtitle = \"Density Plot\") +\n  labs(x = \"BMI\", y = \"Density\", fill = \"Diabetes Status\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\nLast, we have density plot! This density plot visualizes the distribution of BMI values for those with and without diabetes. Here we can see that the peak for both diabetes status are around a BMI of 28, indicating that both groups have most individuals at a BMI of 28. The dashed blue line indicates the mean of around 28 for BMI. When looking at the density for those without diabetes (red), the curve is more tall and narrow compared to the yes diabetes status, indicating that people without diabetes tend to have lower BMIs. Those with diabetes (blue) density curve shifted more to the right indicated that individuals with diabetes tend to have a higher BMI.\n\n\nThat’s a lot of interesting variables and distributions that we’ve looked at! I never really thought about all of the potential variables that could perhaps contribute to or reduce the risk of obtaining diabetes. Now that we’ve completed our EDA, we’re going to create our modeling file that’ll help us predict our diabetes risk.\n\nClick here for the Modeling Page"
  }
]